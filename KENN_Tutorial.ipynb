{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Enhanced Neural Networks - Tutorial Notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is intended as a tutorial for the KENN2 (Knowledge Enhanced Neural Networks) framework. \n",
    "KENN2 (Knowledge Enhanced Neural Networks v2.1) is a library for Python 3 built on top of TensorFlow 2 that allows you to modify neural network models by providing logical knowledge in the form of a set of universally quantified FOL clauses. It does so by allowing the addition of a new final layer (KENN in the picture below) to the existing neural network. Such layer changes the original predictions of the standard neural network, enforcing the satisfaction of the provided knowledge.\n",
    "\n",
    "![title](imgs/KENN_overview.png)\n",
    "\n",
    "In the picture above, we used the symbol `z` to represent the output of the neural network to indicate that the input of KENN must be the preactivations of the predicted truth values. Indeed, KENN layer could be seen as a special type of activation function which enforces the logical knowledge.\n",
    "\n",
    "Notice also that the neural network is represented in gray. This is bacause, in general, KENN is agnostic to the architecture used and the input `z` could be only partially calculated by the neural network. For instance, in this tutorial, the binary predicates are given as inputs since the task is to predict only the unary predicates.\n",
    "\n",
    "To create the KENN layer, the library provides parsers functions, each of which represent a parser for a specific type of knowledge base file. The parser compiles the knowledge and returns the KENN layer, which can be used as a standard TensorFlow operator and can be inserted as a final layer of the model.\n",
    "\n",
    "The main parsers available in KENN are two: `unary_parser` and `relational_parser` (though it is possible to extend the library with new parsers). In the first case, the knowledge contains only unary predicates, and the `x` and `y` of the picture above corresponds to matrices (where rows represent different objects and columns different predicates). In the second case, the `relational_parser` takes as input a knowledge file with both unary and binary predicates and `x` and `y` are (usually) graphs.\n",
    "\n",
    "In this notebook, we focus on the relational case. In particular, here we focus on a simple application of KENN on the Citeseer Dataset, a citation network where the only binary predicate (which correponds the the edges of the graph) represent the citation relation between papers and the goal is to predict the topic of the papers given both the papers features and the citations among them.\n",
    "\n",
    "For a simple explanation of KENN2 usage with the unary parser, please check the README page of the [github page of KENN2](https://github.com/DanieleAlessandro/KENN2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Citeseer Dataset\n",
    "The Citeseer Dataset is essentially a directed graph: the nodes are papers, while an edge between a paper `x` and another paper `y` means that `x` cites `y`. Each paper can be classified into one of six classes, namely: Agents, AI, DB, IR, ML and HCI. Moreover, for each paper a bag of words vector is provided as features.\n",
    "\n",
    "The task is to **correctly classify each scientific publication**, considering both the features of the paper and the relational information provided by the citation network.\n",
    "\n",
    "More specifically, the dataset consists of:\n",
    "- **3312 scientific publications**; \n",
    "- **4732 links**;\n",
    "- Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of **3703 unique words**, which was obtained after stemming and removing stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Test splits\n",
    "\n",
    "For this tutorial, we follow the **Inductive Learning** paradigm when splitting the whole dataset into Training, Validation and Test Set (i.e., only edges between nodes in the same set are used, while edges across the three sets are removed).\n",
    "\n",
    "Specifically, we use $8\\%$ of the nodes (papers) as Training Set, $2\\%$ for validation and the remaining $90\\%$ as a Test Set. This split is particularly relevant since there are not a lot of samples available at training time. In this type of scenario, the additional information coming from the knowledge become particularly relevant since tha data is not enough to learn everything from scratch.\n",
    "\n",
    "More in depth experiments on CiteSeer are available on [GitHub](https://github.com/rmazzier/KENN-Citeseer-Experiments).\n",
    "\n",
    "\n",
    "### The Prior Knowledge\n",
    "\n",
    "The Prior Knowledge we want to inject is pretty simple. Indeed, it is composed of only 6 clauses, one for each topic. More in details, given a topic `T`, we inject the clause\n",
    "\n",
    "$$\\forall x \\forall y \\quad \\lnot T(x) \\lor \\lnot Cite(x,y) \\lor T(y)$$\n",
    "\n",
    "Such rule codifies the idea that papers cite works that are related to them (i.e. the topic of a paper is often the same of the paper it cites). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation\n",
    "\n",
    "Before looking in details the model architecture, we need to specify how to represent a graph in terms of matrices. Picture below shows an example of a graph (left) and the corresponding representation used by KENN2 (right).\n",
    "\n",
    "In the example there are three unary predicates (labels of the nodes) and two binary predicates (red and green arrows). For both unary and binary predicates the numbers represent the preactivations of the truth values. To simplify the picture, many edges are set to false and not showed in the graph. Note that, since KENN layer works with preactivations, the false truth values' preactivations are set to a very small number (-500) in the matrices on the right.\n",
    "\n",
    "![title](imgs/KENN_graph.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KENN2, the graph is represented using two matrices and two indexes vectors:\n",
    "- The `Unary` matrix contains the truth values of unary predicates. In particular, each column corresponds to a specific predicate, while the rows represent the different objects of the domain (the nodes of the graph). To simplify the presentation, an additional column with the objects' indexes has been added to the picture;\n",
    "- The `Binary` matrix has a similar structure, but it contains the truth values of binary predicates. In this case, each row corresponds to a pair of nodes;\n",
    "- Finally, the two vectors `sx` and `sy` contain the indexes of the objects referred by the `Binary` matrix. As an example, in the picture above, the first row of `Binary` contains the binary predicates referring the pairs of nodes 0 and 1. Hence, the values in the first row of `sx` and `sy` contains 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KENN architecture on CiteSeer\n",
    "\n",
    "<a id='architecture'></a>\n",
    "\n",
    "![title](imgs/KENN_architecture.png)\n",
    "\n",
    "The picture above depicts the architecture used in this tutorial. There are four steps, the first two can be seen as a pre-elaboration and they are applied only once:\n",
    "- **Step 1: pre-elaboration of the relational data.** Here the graph is stored using the previously defined representation. Notice that in this case there is only one column in the `Binary` Matrix (`relations` in the picture above). Such column represents the `Cite` predicate and it is setted to 500 for all the pairs. This is because the citations are given as input and we consider only the pairs `(x,y)` for which `Cite(x,y)` is true;\n",
    "- **Step 2: parsing the knowledge file.** Here the knowledge file is provided to the parser, which in turns generates the KENN layer.\n",
    "\n",
    "After the first two steps the model is defined and it can be trained like any neural network in TensorFlow. The model produces its predictions in the last two steps:\n",
    "- **Step 3: initial predictions provided by the neural network.** The NN uses the nodes' features to classify each paper. It returns the preactivations for each paper and each topic;\n",
    "- **Step 4: knowledge injection.** The KENN layer updates the predictions of the NN and returns the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use KENN2\n",
    "\n",
    "## Imports and initial settings\n",
    "\n",
    "Lets now dive into the code, starting from the necessary imports. The first rows are standard python libraries. The only additional import required by KENN2 is the parser function, which allows to read a knowldge file for generating the KENN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "from KENN2.parsers import relational_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix the code behaviour, we also set the random seed for TensorFlow and numpy. Feel free to experiment with different values of the seed or directly remove the entire cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets define the path of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'dataset/CiteSeer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Here we import the relational data. In particular, the three matrices defined in the [KENN architecture on CiteSeer](#architecture) section: `features`, `indexes` and `relations`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features: \n",
    "The features matrix contains a row for each node in the graph, while the columns take values in $\\{0,1\\}$, 0 meaning the absence of a word of the dictionary, 1 meaning its presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = np.genfromtxt(dataset_folder + 'training_features.csv', delimiter=',')\n",
    "validation_features = np.genfromtxt(dataset_folder + 'validation_features.csv', delimiter=',')\n",
    "test_features = np.genfromtxt(dataset_folder + 'test_features.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3693</th>\n",
       "      <th>3694</th>\n",
       "      <th>3695</th>\n",
       "      <th>3696</th>\n",
       "      <th>3697</th>\n",
       "      <th>3698</th>\n",
       "      <th>3699</th>\n",
       "      <th>3700</th>\n",
       "      <th>3701</th>\n",
       "      <th>3702</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 3703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  3693  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1     0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "259   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "260   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "261   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "262   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "263   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "     3694  3695  3696  3697  3698  3699  3700  3701  3702  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "259   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "260   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "261   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "262   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "263   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[264 rows x 3703 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the matrix above seems to contains only zeros just because it is very sparse. As an example, here the indexes of the words contained in the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  32  211  249  383  407  493  507  609  619  731  744 1087 1118 1239\n",
      " 1245 1548 1611 1619 1641 1841 2216 2395 2407 2448 2492 2539 2553 2563\n",
      " 2568 2615 2741 2875 2902 2906 3122 3184 3463 3586 3594]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(training_features[0]==1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_training = np.genfromtxt(dataset_folder + 'indexes_training.csv', delimiter=',', dtype=np.int32)\n",
    "indexes_validation = np.genfromtxt(dataset_folder + 'indexes_validation.csv', delimiter=',', dtype=np.int32)\n",
    "indexes_test = np.genfromtxt(dataset_folder + 'indexes_test.csv', delimiter=',', dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training_indexes matrix: (34, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>193</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>262</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  140  140\n",
       "1   94  139\n",
       "2  127  203\n",
       "3  127  228\n",
       "4  237  237\n",
       "5  193  226\n",
       "6   53   53\n",
       "7   83   83\n",
       "8  262  262\n",
       "9   69   69"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of the training_indexes matrix: \" + str(indexes_training.shape))\n",
    "pd.DataFrame(indexes_training).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that there is a good amount of papers that seemingly cite themselves. This strange behaviour is probably due to the fact that Citeseer was generated with an automatic citation indexing system, which seems not capable to disambiguate between papers having the same author names.\n",
    "\n",
    "Check https://clgiles.ist.psu.edu/papers/DL-1998-citeseer.pdf for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relations: \n",
    "As we explained above, this vector contains the truth values of the connection between two nodes.\n",
    "Note that we just consider the couples of connected nodes, and exclude all the other non connected couples of nodes in the graph. For this reason the only value in the relations matrix is 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT RELATIONS\n",
    "relations_training = np.genfromtxt(dataset_folder + 'relations_training.csv', delimiter=',')\n",
    "relations_validations = np.genfromtxt(dataset_folder + 'relations_validation.csv', delimiter=',')\n",
    "relations_test = np.genfromtxt(dataset_folder + 'relations_test.csv', delimiter=',')\n",
    "\n",
    "# Reshape relations arrays to be column vectors\n",
    "relations_training = np.expand_dims(relations_training, axis=1)\n",
    "relations_validations = np.expand_dims(relations_validations, axis=1)\n",
    "relations_test = np.expand_dims(relations_test, axis=1)\n",
    "\n",
    "n_features = training_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training_relations matrix: (34, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  500.0\n",
       "1  500.0\n",
       "2  500.0\n",
       "3  500.0\n",
       "4  500.0\n",
       "5  500.0\n",
       "6  500.0\n",
       "7  500.0\n",
       "8  500.0\n",
       "9  500.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of the training_relations matrix: \" + str(relations_training.shape))\n",
    "pd.DataFrame(relations_training).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels: \n",
    "Finally, in order to perform training and evaluations, we need also to import the labels. Labels are encoded as a one-hot vector vector of length 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = np.genfromtxt(dataset_folder + 'training_labels.csv', delimiter=',')\n",
    "validation_labels = np.genfromtxt(dataset_folder + 'validation_labels.csv', delimiter=',')\n",
    "test_labels = np.genfromtxt(dataset_folder + 'test_labels.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5\n",
       "0    0.0  0.0  1.0  0.0  0.0  0.0\n",
       "1    1.0  0.0  0.0  0.0  0.0  0.0\n",
       "2    1.0  0.0  0.0  0.0  0.0  0.0\n",
       "3    0.0  0.0  0.0  0.0  1.0  0.0\n",
       "4    0.0  1.0  0.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...\n",
       "259  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "260  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "261  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "262  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "263  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[264 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "\n",
    "Here we define the standard model (a feedforward neural network with three hidden layers) and KENN model (the same architecture as the standard model with the addition of KENN layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard(Model):\n",
    "    def __init__(self):\n",
    "        super(Standard, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.h1 = layers.Dense(50, input_shape=input_shape, activation='relu')\n",
    "        self.d1 = layers.Dropout(0.5)\n",
    "        self.h2 = layers.Dense(50, input_shape=(50,), activation='relu')\n",
    "        self.d2 = layers.Dropout(0.5)\n",
    "        self.h3 = layers.Dense(50, input_shape=(50,), activation='relu')\n",
    "        self.d3 = layers.Dropout(0.5)\n",
    "\n",
    "        self.last_layer = layers.Dense(\n",
    "            6, input_shape=(50,), activation='linear')\n",
    "\n",
    "    def preactivations(self, inputs):\n",
    "        x = self.h1(inputs)\n",
    "        x = self.d1(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        return self.last_layer(x)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        z = self.preactivations(inputs)\n",
    "\n",
    "        return z, softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KENN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kenn(Standard):\n",
    "    def __init__(self, knowledge_file, *args, **kwargs):\n",
    "        super(Kenn, self).__init__(*args, **kwargs)\n",
    "        self.knowledge = knowledge_file\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Kenn, self).build(input_shape)\n",
    "        self.kenn_layer_1 = relational_parser(self.knowledge)\n",
    "        self.kenn_layer_2 = relational_parser(self.knowledge)\n",
    "        self.kenn_layer_3 = relational_parser(self.knowledge, activation=softmax)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        features = inputs[0]\n",
    "        relations = inputs[1]\n",
    "        sx = inputs[2]\n",
    "        sy = inputs[3]\n",
    "        \n",
    "        z = self.preactivations(features)\n",
    "        z, _ = self.kenn_layer_1(z, relations, sx, sy)\n",
    "        z, _ = self.kenn_layer_2(z, relations, sx, sy)\n",
    "        z, _ = self.kenn_layer_3(z, relations, sx, sy)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we defined the KENN model by extending the Standard NN with 3 additional KENN layers. We add three layers instead of one since a single layer considers only the neighbours of each node. By adding three layers we allow for a propagation of the changes to farther nodes.\n",
    "\n",
    "As already mentioned previously, to generate a KENN layer is sufficient to call the `relational_parser` function, providing as input the knowledge file. More in details, the `relational_parser` function parses the knowledge file and returns a ```RelationalKenn``` object which is constructed by looking at the provided knowledge file. Such object can then be used with standard TensorFlow code, as we do below.\n",
    "\n",
    "Notice also that the third layer is created by passing an activation function (the `softmax`) to the parser. This is bacause, internally, the KENN layer just modifies the preactivations provided as input (based on the knowledge) and then applies an activation function (the default one is a linear activation).\n",
    "\n",
    "Lets now take a look at the knowldge base file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_AI,_Agents,_DB,_HCI,_IR,_ML\r",
      "\r\n",
      "Cite\r",
      "\r\n",
      "\r",
      "\r\n",
      ">\r",
      "\r\n",
      "_:n_AI(x),nCite(x.y),_AI(y)\r",
      "\r\n",
      "_:n_Agents(x),nCite(x.y),_Agents(y)\r",
      "\r\n",
      "_:n_DB(x),nCite(x.y),_DB(y)\r",
      "\r\n",
      "_:n_HCI(x),nCite(x.y),_HCI(y)\r",
      "\r\n",
      "_:n_IR(x),nCite(x.y),_IR(y)\r",
      "\r\n",
      "_:n_ML(x),nCite(x.y),_ML(y)\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat knowledge_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row contains the list of unary predicates separated by a comma with no spaces, while the second row contains the binary predicates (in our case, just one). Each predicate should start with a capital letter or another symbol, to ensure that we have no conflicts with other rules we'll define below.\n",
    "\n",
    "The other rows contain the clauses, which are also split in two groups: the first group contains only unary predicates, the second both unary and binary predicates. The two groups are separated by a row containing the > symbol.\n",
    "Each clause is in a separate row and must be written respecting this properties:\n",
    "\n",
    "- Logical disjunctions are represented with commas;\n",
    "- If a literal is negated, it must be preceded by the lowercase 'n';\n",
    "- They must contain only predicates specified in the first row;\n",
    "- There shouldn't be spaces.\n",
    "\n",
    "Additionally, each clause must be preceded by a positive weight that represents the strength of the clause. More precisely, the weight could be a numeric value or an underscore: in the first case, the weight is fixed and determined by the specified value, in the second case the weight is learned during training. \n",
    "\n",
    "Unary predicates are defined on a single variable (e.g. AI(x)) while binary predicates on two variables separated by a dot (e.g. Cite(x.y)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup\n",
    "\n",
    "Lets now set the training hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 300\n",
    "\n",
    "# Early Stopping parameters\n",
    "min_delta = 0.001\n",
    "es_patience = 10\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss = keras.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping:\n",
    "\n",
    "In this tutorial we use Early Stopping which proved to be beneficial when applied to the KENN model.\n",
    "\n",
    "The `callback_early_stopping` function takes as argument the list with all the validation accuracies. \n",
    "If `patience`=$k$, it checks if the mean of the last $k$ accuracies is higher than the mean of the \n",
    "previous $k$ accuracies (i.e. we check that we are not overfitting). If not, it stops learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    correctly_classified = tf.equal(\n",
    "        tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    return tf.reduce_mean(tf.cast(correctly_classified, tf.float32))\n",
    "\n",
    "def callback_early_stopping(AccList, min_delta=min_delta, patience=es_patience):\n",
    "    if len(AccList)//patience < 2:\n",
    "        return False\n",
    "    \n",
    "    mean_previous = np.mean(AccList[::-1][patience:2*patience])\n",
    "    mean_recent = np.mean(AccList[::-1][:patience])\n",
    "    delta = mean_recent - mean_previous\n",
    "\n",
    "    if delta <= min_delta:\n",
    "        print(\n",
    "            \"*CB_ES* Validation Accuracy didn't increase in the last %d epochs\" % (patience))\n",
    "        print(\"*CB_ES* delta:\", delta)\n",
    "    \n",
    "    return delta <= min_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the base NN\n",
    "\n",
    "Here we train the standard NN on its own. Later, we will compare its Test Accuracy with the one of KENN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 13.3542 Validation Loss: 13.4059 | Train Accuracy: 0.3598 Validation Accuracy: 0.2687;\n",
      "Epoch 10: Training Loss: 12.2838 Validation Loss: 13.1026 | Train Accuracy: 0.8674 Validation Accuracy: 0.3881;\n",
      "Epoch 20: Training Loss: 8.8543 Validation Loss: 11.9123 | Train Accuracy: 0.9432 Validation Accuracy: 0.4478;\n",
      "Epoch 30: Training Loss: 3.1537 Validation Loss: 10.3593 | Train Accuracy: 0.9924 Validation Accuracy: 0.5672;\n",
      "Epoch 40: Training Loss: 0.3242 Validation Loss: 9.5506 | Train Accuracy: 1.0000 Validation Accuracy: 0.5075;\n",
      "*CB_ES* Validation Accuracy didn't increase in the last 10 epochs\n",
      "*CB_ES* delta: -0.01791042\n",
      "callback_early_stopping signal received at epoch= 44/300\n",
      "Terminating training \n"
     ]
    }
   ],
   "source": [
    "# Define and build model\n",
    "standard_model = Standard()\n",
    "standard_model.build((n_features,))\n",
    "\n",
    "\n",
    "# Used for early stopping\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, predictions = standard_model(training_features)\n",
    "        training_loss = loss(predictions, training_labels)\n",
    "\n",
    "        gradient = tape.gradient(training_loss, standard_model.variables)\n",
    "        optimizer.apply_gradients(zip(gradient, standard_model.variables))\n",
    "\n",
    "    \n",
    "    _, v_predictions = standard_model(validation_features)\n",
    "    v_accuracy = accuracy(v_predictions, validation_labels)\n",
    "    valid_accuracies.append(v_accuracy.numpy())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        _, t_predictions = standard_model(training_features)\n",
    "        t_loss = loss(t_predictions,training_labels)\n",
    "        t_accuracy = accuracy(t_predictions, training_labels)\n",
    "        \n",
    "        v_loss = loss(v_predictions, validation_labels)\n",
    "\n",
    "        print(\n",
    "            \"Epoch {}: Training Loss: {:5.4f} Validation Loss: {:5.4f} | Train Accuracy: {:5.4f} Validation Accuracy: {:5.4f};\".format(\n",
    "                epoch, t_loss, v_loss, t_accuracy, v_accuracy))\n",
    "\n",
    "\n",
    "    # Early Stopping\n",
    "    stopEarly = callback_early_stopping(valid_accuracies)\n",
    "    if stopEarly:\n",
    "        print(\"callback_early_stopping signal received at epoch= %d/%d\" %\n",
    "                (epoch, n_epochs))\n",
    "        print(\"Terminating training \")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training KENN\n",
    "\n",
    "The code for training KENN is not particularly different from the standard NN model. Indeed, the only difference is in the inputs of the model that were composed of just the nodes' features in the case of the NN and now contain also the relational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 13.3580 Validation Loss: 13.4560 | Train Accuracy: 0.2879 Validation Accuracy: 0.1940;\n",
      "Epoch 10: Training Loss: 11.5904 Validation Loss: 13.1699 | Train Accuracy: 0.8674 Validation Accuracy: 0.2687;\n",
      "Epoch 20: Training Loss: 5.2270 Validation Loss: 11.7052 | Train Accuracy: 0.9886 Validation Accuracy: 0.3284;\n",
      "Epoch 30: Training Loss: 0.3318 Validation Loss: 9.1088 | Train Accuracy: 1.0000 Validation Accuracy: 0.5522;\n",
      "Epoch 40: Training Loss: 0.0201 Validation Loss: 7.6865 | Train Accuracy: 1.0000 Validation Accuracy: 0.6418;\n",
      "Epoch 50: Training Loss: 0.0043 Validation Loss: 7.2665 | Train Accuracy: 1.0000 Validation Accuracy: 0.6567;\n",
      "*CB_ES* Validation Accuracy didn't increase in the last 10 epochs\n",
      "*CB_ES* delta: -5.9604645e-08\n",
      "callback_early_stopping signal received at epoch= 56/300\n",
      "Terminating training \n"
     ]
    }
   ],
   "source": [
    "kenn_model = Kenn('knowledge_base')\n",
    "kenn_model.build((n_features,))\n",
    "\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions_KENN = kenn_model(\n",
    "            [training_features, relations_training, np.expand_dims(indexes_training[:,0], axis=1), np.expand_dims(indexes_training[:,1], axis=1)])\n",
    "\n",
    "        l = loss(predictions_KENN, training_labels)\n",
    "\n",
    "        gradient = tape.gradient(l, kenn_model.variables)\n",
    "        optimizer.apply_gradients(zip(gradient, kenn_model.variables))\n",
    "    \n",
    "    v_predictions = kenn_model([validation_features, relations_validations, np.expand_dims(indexes_validation[:,0], axis=1), np.expand_dims(indexes_validation[:,1], axis=1)])\n",
    "    v_accuracy = accuracy(v_predictions, validation_labels)\n",
    "    valid_accuracies.append(v_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        t_predictions = kenn_model(\n",
    "                [training_features, relations_training, np.expand_dims(indexes_training[:,0], axis=1), np.expand_dims(indexes_training[:,1], axis=1)])\n",
    "        t_loss = loss(t_predictions, training_labels)\n",
    "        t_accuracy = accuracy(t_predictions, training_labels)\n",
    "\n",
    "\n",
    "        v_loss = loss(v_predictions, validation_labels)\n",
    "\n",
    "        print(\n",
    "            \"Epoch {}: Training Loss: {:5.4f} Validation Loss: {:5.4f} | Train Accuracy: {:5.4f} Validation Accuracy: {:5.4f};\".format(\n",
    "                epoch, t_loss, v_loss, t_accuracy, v_accuracy))\n",
    "\n",
    "    # Early Stopping\n",
    "    stopEarly = callback_early_stopping(valid_accuracies)\n",
    "    if stopEarly:\n",
    "        print(\"callback_early_stopping signal received at epoch= %d/%d\" %\n",
    "                (epoch, n_epochs))\n",
    "        print(\"Terminating training \")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set\n",
    "<a id='evaluations'></a>\n",
    "\n",
    "Lets evaluate the NN model with and without the KENN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model Test Accuracy: 52.53271%\n"
     ]
    }
   ],
   "source": [
    "_, predictions_test = standard_model(test_features)\n",
    "test_accuracy = accuracy(predictions_test, test_labels)\n",
    "print(\"Standard model Test Accuracy: {:.5f}%\".format(test_accuracy.numpy() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KENN model Test Accuracy: 61.15398%\n"
     ]
    }
   ],
   "source": [
    "ind_x = np.expand_dims(indexes_test[:,0], axis=1)\n",
    "ind_y = np.expand_dims(indexes_test[:,1], axis=1)\n",
    "\n",
    "predictions_test_kenn = kenn_model(\n",
    "    [test_features, relations_test, ind_x, ind_y])\n",
    "\n",
    "test_accuracy_kenn = accuracy(predictions_test_kenn, test_labels)\n",
    "print(\"KENN model Test Accuracy: {:.5f}%\".format(test_accuracy_kenn.numpy() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are quite larger in the second case. This improvement on the test accuracy can be explained as the results of adding the knowledge, since the two models differences are only in the final KENN layers.\n",
    "\n",
    "Notice that it could happen that KENN does not improve the base neural networks accuracy due to random fluctuations. In the following image, some histograms that show the distributions of KENN results on 500 runs (with different splits of the dataset).\n",
    "\n",
    "Each row corresponds to a different amount of training data (the first, like in this tutorial, is with 10% of the data for training and validation).\n",
    "\n",
    "The left column contains the distributions of results of the NN and KENN models, while the second column the distributions of the improvements provided by KENN.\n",
    "\n",
    "\n",
    "![CiteSeer Experiments](https://raw.githubusercontent.com/rmazzier/KENN-Citeseer-Experiments/main/plots/history_inductive.png)\n",
    "\n",
    "For more information on the CiteSeer experiments, please visit the [GitHub](https://github.com/rmazzier/KENN-Citeseer-Experiments) page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KENN in summary\n",
    "\n",
    "To summarize, there are few changes to standard TensorFlow code needed to incorporate the knowledge with KENN2:\n",
    "- importing of a parser;\n",
    "- calling the parser with the knowledge file path;\n",
    "- using the resulting layer as a function on top of the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}