{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Enhanced Neural Networks - Tutorial Notebook\n",
    "With this notebook we present a simple application of KENN on the Citeseer Dataset, where relational logical knowledge is employed to improve the predictions of a baseline NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from KENN2.parsers import relational_parser\n",
    "from tensorflow.keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET RANDOM SEED for tensorflow and numpy\n",
    "random_seed = 0\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Citeseer Dataset\n",
    "The Citeseer Dataset consists of \n",
    "- **3312 scientific publications** classified into one of six classes. \n",
    "- The citation network consists of **4732 links**. \n",
    "- Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of **3703 unique words**. \n",
    "\n",
    "The task is to **correctly classify each scientific publication**, given in input the features for each sample and the relational information provided by the citation network.\n",
    "\n",
    "Here we use the **Inductive** paradigm: we consider only the edges (x,y) such that both x and y are in the Training/Validation/Test Set;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation\n",
    "![title](imgs/data_repr.png)\n",
    "We represent all the features as a matrix with $n$ rows and $k$ columns, where $n$ is the number of nodes and $k$ is the number of features.\n",
    "\n",
    "The relational data (i.e. the relations between the nodes of the network) are summarized in a table of index couples: for each edge in the network, we store the index of the first node and the index of the second node in a single row of the **indexes** matrix. The **relations** table contains the truth value of each ordered couple in the indexes matrix; note that we consider only couples of connected nodes. Furthermore, note that with \"truth value\" we mean a number in the range $[-\\infty, \\infty]$, since, inside the architecture of KENN, those are considered as the preactivations of the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT FEATURES\n",
    "training_features = np.genfromtxt('dataset/CiteSeer/training_features.csv', delimiter=',')\n",
    "validation_features = np.genfromtxt('dataset/CiteSeer/validation_features.csv', delimiter=',')\n",
    "test_features = np.genfromtxt('dataset/CiteSeer/test_features.csv', delimiter=',')\n",
    "\n",
    "# IMPORT LABELS\n",
    "training_labels = np.genfromtxt('dataset/CiteSeer/training_labels.csv', delimiter=',')\n",
    "validation_labels = np.genfromtxt('dataset/CiteSeer/validation_labels.csv', delimiter=',')\n",
    "test_labels = np.genfromtxt('dataset/CiteSeer/test_labels.csv', delimiter=',')\n",
    "\n",
    "# IMPORT EDGES INDEXES\n",
    "indexes_training = np.genfromtxt('dataset/CiteSeer/indexes_training.csv', delimiter=',', dtype=int)\n",
    "indexes_validation = np.genfromtxt('dataset/CiteSeer/indexes_validation.csv', delimiter=',', dtype=int)\n",
    "indexes_test = np.genfromtxt('dataset/CiteSeer/indexes_test.csv', delimiter=',', dtype=int)\n",
    "\n",
    "# IMPORT RELATIONS\n",
    "relations_training = np.genfromtxt('dataset/Citeseer/relations_training.csv', delimiter=',')\n",
    "relations_validations = np.genfromtxt('dataset/CiteSeer/relations_validation.csv', delimiter=',')\n",
    "relations_test = np.genfromtxt('dataset/Citeseer/relations_test.csv', delimiter=',')\n",
    "\n",
    "# make relations arrays column vectors\n",
    "relations_training = np.expand_dims(relations_training, axis=1)\n",
    "relations_validations = np.expand_dims(relations_validations, axis=1)\n",
    "relations_test = np.expand_dims(relations_test, axis=1)\n",
    "\n",
    "n_features = training_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "![title](imgs/experiment_setup.png)\n",
    "\n",
    "\n",
    "In this example, the relational data is injected directly from the citation network. KENN uses this data to increase the truth value of each clause that is given as input in the prior knowledge.\n",
    "Specifically, in this case, the knowledge codifies the idea that papers cite works that are related to them (i.e. the topic of a paper is often the same of the paper it cites). \n",
    "\n",
    "For this reason we instantiate the clause:\n",
    "$$\\forall x \\forall y \\quad T(x) \\land Cite(x,y) \\rightarrow T(y)$$\n",
    "multiple times, for all the topics $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "Here we define a Standard Sequential Model, and a Relational KENN model with Tensorflow Subclassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard(Model):\n",
    "    def __init__(self):\n",
    "        super(Standard, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.h1 = layers.Dense(50, input_shape=input_shape, activation='relu')\n",
    "        self.d1 = layers.Dropout(0.5)\n",
    "        self.h2 = layers.Dense(50, input_shape=(50,), activation='relu')\n",
    "        self.d2 = layers.Dropout(0.5)\n",
    "        self.h3 = layers.Dense(50, input_shape=(50,), activation='relu')\n",
    "        self.d3 = layers.Dropout(0.5)\n",
    "\n",
    "        self.last_layer = layers.Dense(\n",
    "            6, input_shape=(50,), activation='linear')\n",
    "\n",
    "    def preactivations(self, inputs):\n",
    "        x = self.h1(inputs)\n",
    "        x = self.d1(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        return self.last_layer(x)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        z = self.preactivations(inputs)\n",
    "\n",
    "        return z, softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kenn(Standard):\n",
    "    \"\"\"\n",
    "    Model with 3 KENN layers.\n",
    "    for each individual clause enhancer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, knowledge_file, *args, **kwargs):\n",
    "        super(Kenn, self).__init__(*args, **kwargs)\n",
    "        self.knowledge = knowledge_file\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Kenn, self).build(input_shape)\n",
    "        self.kenn_layer_1 = relational_parser(self.knowledge)\n",
    "        self.kenn_layer_2 = relational_parser(self.knowledge)\n",
    "        self.kenn_layer_3 = relational_parser(self.knowledge)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        features = inputs[0]\n",
    "        relations = inputs[1]\n",
    "        sx = inputs[2]\n",
    "        sy = inputs[3]\n",
    "\n",
    "        z = self.preactivations(features)\n",
    "        z, _ = self.kenn_layer_1(z, relations, sx, sy)\n",
    "        z, _ = self.kenn_layer_2(z, relations, sx, sy)\n",
    "        z, _ = self.kenn_layer_3(z, relations, sx, sy)\n",
    "\n",
    "        return softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "verbose = True\n",
    "n_epochs = 300\n",
    "\n",
    "# Early Stopping parameters\n",
    "min_delta = 0.001\n",
    "es_patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    correctly_classified = tf.equal(\n",
    "        tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    return tf.reduce_mean(tf.cast(correctly_classified, tf.float32))\n",
    "\n",
    "def callback_early_stopping(AccList, min_delta=min_delta, patience=es_patience):\n",
    "    \"\"\"\n",
    "    Takes as argument the list with all the validation accuracies. \n",
    "    If patience=k, checks if the mean of the last k accuracies is higher than the mean of the \n",
    "    previous k accuracies (i.e. we check that we are not overfitting). If not, stops learning.\n",
    "    \"\"\"\n",
    "    # No early stopping for 2*patience epochs\n",
    "    if len(AccList)//patience < 2:\n",
    "        return False\n",
    "    # Mean loss for last patience epochs and second-last patience epochs\n",
    "    mean_previous = np.mean(AccList[::-1][patience:2*patience])\n",
    "    mean_recent = np.mean(AccList[::-1][:patience])\n",
    "    delta = mean_recent - mean_previous\n",
    "\n",
    "    if delta <= min_delta:\n",
    "        print(\n",
    "            \"*CB_ES* Validation Accuracy didn't increase in the last %d epochs\" % (patience))\n",
    "        print(\"*CB_ES* delta:\", delta)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the base NN\n",
    "def train_standard_nn():\n",
    "    \"\"\"\n",
    "    Trains Standard model with the Training Set, validates on Validation Set\n",
    "    and evaluates accuracy on the Test Set.\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    loss = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # TRAIN AND EVALUATE STANDARD MODEL\n",
    "    for epoch in range(n_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            _, predictions = standard_model(training_features)\n",
    "            training_loss = loss(predictions, training_labels)\n",
    "\n",
    "            gradient = tape.gradient(training_loss, standard_model.variables)\n",
    "            optimizer.apply_gradients(zip(gradient, standard_model.variables))\n",
    "\n",
    "\n",
    "        _, t_predictions = standard_model(training_features)\n",
    "        t_loss = loss(t_predictions,training_labels)\n",
    "\n",
    "        _, v_predictions = standard_model(validation_features)\n",
    "        v_loss = loss(v_predictions, validation_labels)\n",
    "\n",
    "        train_losses.append(t_loss.numpy())\n",
    "        valid_losses.append(v_loss.numpy())\n",
    "\n",
    "        t_accuracy = accuracy(t_predictions, training_labels)\n",
    "        v_accuracy = accuracy(v_predictions, validation_labels)\n",
    "\n",
    "        train_accuracies.append(t_accuracy.numpy())\n",
    "        valid_accuracies.append(v_accuracy.numpy())\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch {}: Training Loss: {:5.4f} Validation Loss: {:5.4f} | Train Accuracy: {:5.4f} Validation Accuracy: {:5.4f};\".format(\n",
    "                    epoch, t_loss, v_loss, t_accuracy, v_accuracy))\n",
    "        \n",
    "        # Early Stopping\n",
    "        stopEarly = callback_early_stopping(valid_accuracies)\n",
    "        if stopEarly:\n",
    "            print(\"callback_early_stopping signal received at epoch= %d/%d\" %\n",
    "                    (epoch, n_epochs))\n",
    "            print(\"Terminating training \")\n",
    "            break\n",
    "\n",
    "    return (train_losses,\n",
    "            valid_losses,\n",
    "            valid_accuracies,\n",
    "            train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 13.3542 Validation Loss: 13.4059 | Train Accuracy: 0.3598 Validation Accuracy: 0.2687;\n",
      "Epoch 10: Training Loss: 12.2838 Validation Loss: 13.1026 | Train Accuracy: 0.8674 Validation Accuracy: 0.3881;\n",
      "Epoch 20: Training Loss: 8.8543 Validation Loss: 11.9123 | Train Accuracy: 0.9432 Validation Accuracy: 0.4478;\n",
      "Epoch 30: Training Loss: 3.1537 Validation Loss: 10.3593 | Train Accuracy: 0.9924 Validation Accuracy: 0.5672;\n",
      "Epoch 40: Training Loss: 0.3242 Validation Loss: 9.5506 | Train Accuracy: 1.0000 Validation Accuracy: 0.5075;\n",
      "*CB_ES* Validation Accuracy didn't increase in the last 10 epochs\n",
      "*CB_ES* delta: -0.01791042\n",
      "callback_early_stopping signal received at epoch= 44/300\n",
      "Terminating training \n"
     ]
    }
   ],
   "source": [
    "# Define and build model\n",
    "standard_model = Standard()\n",
    "standard_model.build((n_features,))\n",
    "\n",
    "#Train\n",
    "history_standard = train_standard_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_standard[3], label='Train Accuracy')\n",
    "# plt.plot(history_standard[2], label='Validation Accuracy')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model Test Accuracy: 52.53271%\n"
     ]
    }
   ],
   "source": [
    "_, predictions_test = standard_model(test_features)\n",
    "test_accuracy = accuracy(predictions_test, test_labels)\n",
    "print(\"Standard model Test Accuracy: {:.5f}%\".format(test_accuracy.numpy() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAIN KENN model\n",
    "def train_kenn():\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    loss = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            predictions_KENN = kenn_model(\n",
    "                [training_features, relations_training, np.expand_dims(indexes_training[:,0], axis=1), np.expand_dims(indexes_training[:,1], axis=1)])\n",
    "\n",
    "            l = loss(predictions_KENN, training_labels)\n",
    "\n",
    "            gradient = tape.gradient(l, kenn_model.variables)\n",
    "            optimizer.apply_gradients(zip(gradient, kenn_model.variables))\n",
    "\n",
    "        t_predictions = kenn_model(\n",
    "                [training_features, relations_training, np.expand_dims(indexes_training[:,0], axis=1), np.expand_dims(indexes_training[:,1], axis=1)])\n",
    "        t_loss = loss(t_predictions, training_labels)\n",
    "\n",
    "\n",
    "\n",
    "        v_predictions = kenn_model([validation_features, relations_validations, np.expand_dims(indexes_validation[:,0], axis=1), np.expand_dims(indexes_validation[:,1], axis=1)])\n",
    "        v_loss = loss(v_predictions, validation_labels)\n",
    "\n",
    "        train_losses.append(t_loss)\n",
    "        valid_losses.append(v_loss)\n",
    "\n",
    "        t_accuracy = accuracy(t_predictions, training_labels)\n",
    "        v_accuracy = accuracy(v_predictions, validation_labels)\n",
    "\n",
    "        train_accuracies.append(t_accuracy)\n",
    "        valid_accuracies.append(v_accuracy)\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch {}: Training Loss: {:5.4f} Validation Loss: {:5.4f} | Train Accuracy: {:5.4f} Validation Accuracy: {:5.4f};\".format(\n",
    "                    epoch, t_loss, v_loss, t_accuracy, v_accuracy))\n",
    "\n",
    "            # Early Stopping\n",
    "        stopEarly = callback_early_stopping(valid_accuracies)\n",
    "        if stopEarly:\n",
    "            print(\"callback_early_stopping signal received at epoch= %d/%d\" %\n",
    "                    (epoch, n_epochs))\n",
    "            print(\"Terminating training \")\n",
    "            break\n",
    "    return (train_losses,\n",
    "        valid_losses,\n",
    "        valid_accuracies,\n",
    "        train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 13.3305 Validation Loss: 13.4524 | Train Accuracy: 0.3220 Validation Accuracy: 0.1940;\n",
      "Epoch 10: Training Loss: 12.1357 Validation Loss: 13.2729 | Train Accuracy: 0.8561 Validation Accuracy: 0.2836;\n",
      "Epoch 20: Training Loss: 8.8251 Validation Loss: 12.6146 | Train Accuracy: 0.9508 Validation Accuracy: 0.2836;\n",
      "Epoch 30: Training Loss: 3.4365 Validation Loss: 11.2155 | Train Accuracy: 1.0000 Validation Accuracy: 0.4627;\n",
      "Epoch 40: Training Loss: 0.3959 Validation Loss: 9.3138 | Train Accuracy: 1.0000 Validation Accuracy: 0.5821;\n",
      "Epoch 50: Training Loss: 0.0516 Validation Loss: 8.1978 | Train Accuracy: 1.0000 Validation Accuracy: 0.6418;\n",
      "Epoch 60: Training Loss: 0.0153 Validation Loss: 7.8132 | Train Accuracy: 1.0000 Validation Accuracy: 0.6269;\n",
      "*CB_ES* Validation Accuracy didn't increase in the last 10 epochs\n",
      "*CB_ES* delta: -1.1920929e-07\n",
      "callback_early_stopping signal received at epoch= 64/300\n",
      "Terminating training \n"
     ]
    }
   ],
   "source": [
    "kenn_model = Kenn('knowledge_base')\n",
    "kenn_model.build((n_features,))\n",
    "\n",
    "history_kenn = train_kenn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KENN model Test Accuracy: 62.05971%\n"
     ]
    }
   ],
   "source": [
    "ind_x = np.expand_dims(indexes_test[:,0], axis=1)\n",
    "ind_y = np.expand_dims(indexes_test[:,1], axis=1)\n",
    "\n",
    "predictions_test_kenn = kenn_model(\n",
    "    [test_features, relations_test, ind_x, ind_y])\n",
    "\n",
    "test_accuracy_kenn = accuracy(predictions_test_kenn, test_labels)\n",
    "print(\"KENN model Test Accuracy: {:.5f}%\".format(test_accuracy_kenn.numpy() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_kenn[3], label='Train Accuracy')\n",
    "# plt.plot(history_kenn[2], label='Validation Accuracy')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
