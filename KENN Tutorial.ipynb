{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Enhanced Neural Networks - Tutorial Notebook\n",
    "With this notebook we present a simple application of KENN on the Citeseer Dataset, where relational logical knowledge is employed to improve the predictions of a baseline NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from KENN2.parsers import relational_parser\n",
    "from tensorflow.keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'dataset/CiteSeer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seed for tensorflow and numpy\n",
    "random_seed = 0\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Citeseer Dataset\n",
    "The Citeseer Dataset is essentially a directed graph: the nodes are documents, while the edges are directed in such a way that the tail node is the paper that makes the citation and the head node is the cited paper. More specifically, it consists of:\n",
    "- **3312 scientific publications** classified into one of six classes (namely: Agents, AI, DB, IR, ML, HCI). \n",
    "- The citation network consists of **4732 links**. \n",
    "- Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of **3703 unique words**, which was obtained after stemming and removing stopwords. \n",
    "\n",
    "The task is to **correctly classify each scientific publication**, given in input the features for each sample and the relational information provided by the citation network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation\n",
    "![title](imgs/data_repr.png)\n",
    "We represent all the features as a matrix with $n$ rows and $k$ columns, where $n$ is the number of nodes and $k$ is the number of features.\n",
    "\n",
    "The relational data (i.e. the relations between the nodes of the network) are summarized in a table of index couples: for each edge in the network, we store the index of the first node and the index of the second node in a single row of the **indexes** matrix. The **relations** table contains the truth value of each ordered couple in the indexes matrix; note that we consider only couples of connected nodes. Furthermore, note that with \"truth value\" we mean a number in the range $[-\\infty, \\infty]$, since, inside the architecture of KENN, those are considered as the preactivations of the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Training, Validation and Test splits\n",
    "We split the whole dataset into Training, Validation and Test set. Specifically, we just take $10\\%$ of the complete dataset as Training Set, and use $20\\%$ of it for validation. We use the remaining $90\\%$ as a Test set. We chose to pick this precise split ratio since it's the one where KENN can be more useful. In fact, we observed that when few data is present, KENN gives much better results compared to a standard NN, thanks to the addition of logical knowledge.\n",
    "\n",
    " When making splits, we used the **Inductive** paradigm: we considered only the edges $(x,y)$ such that both $x$ and $y$ are in the same split.\n",
    "For each the following files will be used throughout this notebook:\n",
    "- **features**: matrix containing the feature vectors for each node in the selected split;\n",
    "- **labels**: matrix of one-hot encoded labels for each node in the selected split;\n",
    "- **indexes**: matrix containing all the couples of nodes that are connected by an edge in the selected split;\n",
    "- **relations**: vector, of length equal to the number of rows of the indexes matrix. It contains the truth value of the connection between the corresponding couple of nodes. Since the edges of the graph are not weighted, the possible values for a connection are just \"connected\" and \"not connected\". As stated above, the truth values can be seen as preactivations $z$ of a sigmoid function $\\sigma(z)$. For this reason we just set the truth value of all connected pairs to 500, since $\\sigma(500) \\approx 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT FEATURES\n",
    "training_features = np.genfromtxt(dataset_folder + 'training_features.csv', delimiter=',')\n",
    "validation_features = np.genfromtxt(dataset_folder + 'validation_features.csv', delimiter=',')\n",
    "test_features = np.genfromtxt(dataset_folder + 'test_features.csv', delimiter=',')\n",
    "\n",
    "# IMPORT LABELS\n",
    "training_labels = np.genfromtxt(dataset_folder + 'training_labels.csv', delimiter=',')\n",
    "validation_labels = np.genfromtxt(dataset_folder + 'validation_labels.csv', delimiter=',')\n",
    "test_labels = np.genfromtxt(dataset_folder + 'test_labels.csv', delimiter=',')\n",
    "\n",
    "# IMPORT EDGES INDEXES\n",
    "indexes_training = np.genfromtxt(dataset_folder + 'indexes_training.csv', delimiter=',', dtype=np.int32)\n",
    "indexes_validation = np.genfromtxt(dataset_folder + 'indexes_validation.csv', delimiter=',', dtype=np.int32)\n",
    "indexes_test = np.genfromtxt(dataset_folder + 'indexes_test.csv', delimiter=',', dtype=np.int32)\n",
    "\n",
    "# IMPORT RELATIONS\n",
    "relations_training = np.genfromtxt(dataset_folder + 'relations_training.csv', delimiter=',')\n",
    "relations_validations = np.genfromtxt(dataset_folder + 'relations_validation.csv', delimiter=',')\n",
    "relations_test = np.genfromtxt(dataset_folder + 'relations_test.csv', delimiter=',')\n",
    "\n",
    "# Reshape relations arrays to be column vectors\n",
    "relations_training = np.expand_dims(relations_training, axis=1)\n",
    "relations_validations = np.expand_dims(relations_validations, axis=1)\n",
    "relations_test = np.expand_dims(relations_test, axis=1)\n",
    "\n",
    "n_features = training_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features: \n",
    "Each row is a node in the graph, while each column takes values in $\\{0,1\\}$, the first meaning the absence and the second meaning the presence of the corresponding word in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3693</th>\n",
       "      <th>3694</th>\n",
       "      <th>3695</th>\n",
       "      <th>3696</th>\n",
       "      <th>3697</th>\n",
       "      <th>3698</th>\n",
       "      <th>3699</th>\n",
       "      <th>3700</th>\n",
       "      <th>3701</th>\n",
       "      <th>3702</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 3703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  3693  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1     0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "259   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "260   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "261   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "262   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "263   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "     3694  3695  3696  3697  3698  3699  3700  3701  3702  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "259   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "260   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "261   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "262   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "263   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[264 rows x 3703 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: those are the indexes of the words contained in the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  32  211  249  383  407  493  507  609  619  731  744 1087 1118 1239\n",
      " 1245 1548 1611 1619 1641 1841 2216 2395 2407 2448 2492 2539 2553 2563\n",
      " 2568 2615 2741 2875 2902 2906 3122 3184 3463 3586 3594]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(training_features[0]==1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels: \n",
    "As we can see, each label is one-hot encoded into a vector of length 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5\n",
       "0    0.0  0.0  1.0  0.0  0.0  0.0\n",
       "1    1.0  0.0  0.0  0.0  0.0  0.0\n",
       "2    1.0  0.0  0.0  0.0  0.0  0.0\n",
       "3    0.0  0.0  0.0  0.0  1.0  0.0\n",
       "4    0.0  1.0  0.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...\n",
       "259  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "260  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "261  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "262  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "263  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[264 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's class: DB\n",
      "Document 1's class: Agents\n",
      "Document 2's class: Agents\n",
      "Document 3's class: ML\n",
      "Document 4's class: AI\n"
     ]
    }
   ],
   "source": [
    "# Example: this is the class to which the first 5 samples belong to:\n",
    "topics = [\"Agents\", \"AI\", \"DB\", \"IR\", \"ML\", \"HCI\"]\n",
    "for i in range(5):\n",
    "    print(\"Document {}'s class: \".format(i) + topics[np.where(training_labels[i]==1)[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexes: \n",
    "Each row represents a connection between the first and the second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training_indexes matrix: (34, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>193</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>262</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  140  140\n",
       "1   94  139\n",
       "2  127  203\n",
       "3  127  228\n",
       "4  237  237\n",
       "5  193  226\n",
       "6   53   53\n",
       "7   83   83\n",
       "8  262  262\n",
       "9   69   69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of the training_indexes matrix: \" + str(indexes_training.shape))\n",
    "pd.DataFrame(indexes_training).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that there is a good amount of papers that seemingly cite themselves. This strange behaviour is probably due to the fact that Citeseer is an automatic citation indexing system, which seems not capable to disambiguate between papers having the same author names.\n",
    "\n",
    "Check https://clgiles.ist.psu.edu/papers/DL-1998-citeseer.pdf for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relations: \n",
    "As we explained above, this vector contains the truth values of the connection between two nodes.\n",
    "Note that we just consider the couples of connected nodes, and exclude all the other non connected couples of nodes in the graph. For this reason the unique value in the relations vector is 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training_relations matrix: (34, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  500.0\n",
       "1  500.0\n",
       "2  500.0\n",
       "3  500.0\n",
       "4  500.0\n",
       "5  500.0\n",
       "6  500.0\n",
       "7  500.0\n",
       "8  500.0\n",
       "9  500.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of the training_relations matrix: \" + str(relations_training.shape))\n",
    "pd.DataFrame(relations_training).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "![title](imgs/experiment_setup.png)\n",
    "\n",
    "In this example, the relational data is injected directly from the citation network. KENN uses this data to increase the truth value of each clause that is given as input in the prior knowledge.\n",
    "Specifically, in this case, the knowledge used by KENN codifies the idea that papers cite works that are related to them (i.e. the topic of a paper is often the same of the paper it cites). \n",
    "\n",
    "For this reason we instantiate the clause:\n",
    "$$\\forall x \\forall y \\quad T(x) \\land Cite(x,y) \\rightarrow T(y)$$\n",
    "multiple times, for all the topics $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "Here we define a Standard Sequential Model, and a Relational KENN model with Tensorflow Subclassing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard(Model):\n",
    "    def __init__(self):\n",
    "        super(Standard, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.h1 = layers.Dense(50, input_shape=input_shape, activation='relu')\n",
    "        self.d1 = layers.Dropout(0.5)\n",
    "        self.h2 = layers.Dense(50, input_shape=(50,), activation='relu')\n",
    "        self.d2 = layers.Dropout(0.5)\n",
    "        self.h3 = layers.Dense(50, input_shape=(50,), activation='relu')\n",
    "        self.d3 = layers.Dropout(0.5)\n",
    "\n",
    "        self.last_layer = layers.Dense(\n",
    "            6, input_shape=(50,), activation='linear')\n",
    "\n",
    "    def preactivations(self, inputs):\n",
    "        x = self.h1(inputs)\n",
    "        x = self.d1(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        return self.last_layer(x)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        z = self.preactivations(inputs)\n",
    "\n",
    "        return z, softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KENN Model:\n",
    "Here we define our KENN model, which extends the Standard NN by adding 3 KENN layers. \n",
    "\n",
    "In fact, combining more layers when using data in the form of a graph, can allow us to propagate the logical knowledge not only to adjacent nodes, but also to neighbors of neighbors, and so on. Furthermore, since KENN can learn clause weights, it can also learn the importance to give to not directly adjacent nodes by simply penalizing the clause weights of successive layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kenn(Standard):\n",
    "    def __init__(self, knowledge_file, *args, **kwargs):\n",
    "        super(Kenn, self).__init__(*args, **kwargs)\n",
    "        self.knowledge = knowledge_file\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Kenn, self).build(input_shape)\n",
    "        self.kenn_layer_1 = relational_parser(self.knowledge)\n",
    "        self.kenn_layer_2 = relational_parser(self.knowledge)\n",
    "        self.kenn_layer_3 = relational_parser(self.knowledge)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        features = inputs[0]\n",
    "        relations = inputs[1]\n",
    "        sx = inputs[2]\n",
    "        sy = inputs[3]\n",
    "        \n",
    "        z = self.preactivations(features)\n",
    "        z, _ = self.kenn_layer_1(z, relations, sx, sy)\n",
    "        z, _ = self.kenn_layer_2(z, relations, sx, sy)\n",
    "        z, _ = self.kenn_layer_3(z, relations, sx, sy)\n",
    "\n",
    "        return softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 300\n",
    "\n",
    "# Early Stopping parameters\n",
    "min_delta = 0.001\n",
    "es_patience = 10\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss = keras.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping:\n",
    "Our Early Stopping function takes as argument the list with all the validation accuracies. \n",
    "If patience=$k$, checks if the mean of the last $k$ accuracies is higher than the mean of the \n",
    "previous $k$ accuracies (i.e. we check that we are not overfitting). If not, stops learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    correctly_classified = tf.equal(\n",
    "        tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    return tf.reduce_mean(tf.cast(correctly_classified, tf.float32))\n",
    "\n",
    "def callback_early_stopping(AccList, min_delta=min_delta, patience=es_patience):\n",
    "    if len(AccList)//patience < 2:\n",
    "        return False\n",
    "    \n",
    "    mean_previous = np.mean(AccList[::-1][patience:2*patience])\n",
    "    mean_recent = np.mean(AccList[::-1][:patience])\n",
    "    delta = mean_recent - mean_previous\n",
    "\n",
    "    if delta <= min_delta:\n",
    "        print(\n",
    "            \"*CB_ES* Validation Accuracy didn't increase in the last %d epochs\" % (patience))\n",
    "        print(\"*CB_ES* delta:\", delta)\n",
    "    \n",
    "    return delta <= min_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the base NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 13.3542 Validation Loss: 13.4059 | Train Accuracy: 0.3598 Validation Accuracy: 0.2687;\n",
      "Epoch 10: Training Loss: 12.2838 Validation Loss: 13.1026 | Train Accuracy: 0.8674 Validation Accuracy: 0.3881;\n",
      "Epoch 20: Training Loss: 8.8543 Validation Loss: 11.9123 | Train Accuracy: 0.9432 Validation Accuracy: 0.4478;\n",
      "Epoch 30: Training Loss: 3.1537 Validation Loss: 10.3593 | Train Accuracy: 0.9924 Validation Accuracy: 0.5672;\n",
      "Epoch 40: Training Loss: 0.3242 Validation Loss: 9.5506 | Train Accuracy: 1.0000 Validation Accuracy: 0.5075;\n",
      "*CB_ES* Validation Accuracy didn't increase in the last 10 epochs\n",
      "*CB_ES* delta: -0.01791042\n",
      "callback_early_stopping signal received at epoch= 44/300\n",
      "Terminating training \n"
     ]
    }
   ],
   "source": [
    "# Define and build model\n",
    "standard_model = Standard()\n",
    "standard_model.build((n_features,))\n",
    "\n",
    "\n",
    "# Used for early stopping\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, predictions = standard_model(training_features)\n",
    "        training_loss = loss(predictions, training_labels)\n",
    "\n",
    "        gradient = tape.gradient(training_loss, standard_model.variables)\n",
    "        optimizer.apply_gradients(zip(gradient, standard_model.variables))\n",
    "\n",
    "    \n",
    "    _, v_predictions = standard_model(validation_features)\n",
    "    v_accuracy = accuracy(v_predictions, validation_labels)\n",
    "    valid_accuracies.append(v_accuracy.numpy())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        _, t_predictions = standard_model(training_features)\n",
    "        t_loss = loss(t_predictions,training_labels)\n",
    "        t_accuracy = accuracy(t_predictions, training_labels)\n",
    "        \n",
    "        v_loss = loss(v_predictions, validation_labels)\n",
    "\n",
    "        print(\n",
    "            \"Epoch {}: Training Loss: {:5.4f} Validation Loss: {:5.4f} | Train Accuracy: {:5.4f} Validation Accuracy: {:5.4f};\".format(\n",
    "                epoch, t_loss, v_loss, t_accuracy, v_accuracy))\n",
    "\n",
    "\n",
    "    # Early Stopping\n",
    "    stopEarly = callback_early_stopping(valid_accuracies)\n",
    "    if stopEarly:\n",
    "        print(\"callback_early_stopping signal received at epoch= %d/%d\" %\n",
    "                (epoch, n_epochs))\n",
    "        print(\"Terminating training \")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training KENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 13.3580 Validation Loss: 13.4560 | Train Accuracy: 0.2879 Validation Accuracy: 0.1940;\n",
      "Epoch 10: Training Loss: 11.5904 Validation Loss: 13.1699 | Train Accuracy: 0.8674 Validation Accuracy: 0.2687;\n",
      "Epoch 20: Training Loss: 5.2270 Validation Loss: 11.7052 | Train Accuracy: 0.9886 Validation Accuracy: 0.3284;\n",
      "Epoch 30: Training Loss: 0.3318 Validation Loss: 9.1088 | Train Accuracy: 1.0000 Validation Accuracy: 0.5522;\n",
      "Epoch 40: Training Loss: 0.0201 Validation Loss: 7.6865 | Train Accuracy: 1.0000 Validation Accuracy: 0.6418;\n",
      "Epoch 50: Training Loss: 0.0043 Validation Loss: 7.2665 | Train Accuracy: 1.0000 Validation Accuracy: 0.6567;\n",
      "*CB_ES* Validation Accuracy didn't increase in the last 10 epochs\n",
      "*CB_ES* delta: -5.9604645e-08\n",
      "callback_early_stopping signal received at epoch= 56/300\n",
      "Terminating training \n"
     ]
    }
   ],
   "source": [
    "kenn_model = Kenn('knowledge_base')\n",
    "kenn_model.build((n_features,))\n",
    "\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions_KENN = kenn_model(\n",
    "            [training_features, relations_training, np.expand_dims(indexes_training[:,0], axis=1), np.expand_dims(indexes_training[:,1], axis=1)])\n",
    "\n",
    "        l = loss(predictions_KENN, training_labels)\n",
    "\n",
    "        gradient = tape.gradient(l, kenn_model.variables)\n",
    "        optimizer.apply_gradients(zip(gradient, kenn_model.variables))\n",
    "    \n",
    "    v_predictions = kenn_model([validation_features, relations_validations, np.expand_dims(indexes_validation[:,0], axis=1), np.expand_dims(indexes_validation[:,1], axis=1)])\n",
    "    v_accuracy = accuracy(v_predictions, validation_labels)\n",
    "    valid_accuracies.append(v_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        t_predictions = kenn_model(\n",
    "                [training_features, relations_training, np.expand_dims(indexes_training[:,0], axis=1), np.expand_dims(indexes_training[:,1], axis=1)])\n",
    "        t_loss = loss(t_predictions, training_labels)\n",
    "        t_accuracy = accuracy(t_predictions, training_labels)\n",
    "\n",
    "\n",
    "        v_loss = loss(v_predictions, validation_labels)\n",
    "\n",
    "        print(\n",
    "            \"Epoch {}: Training Loss: {:5.4f} Validation Loss: {:5.4f} | Train Accuracy: {:5.4f} Validation Accuracy: {:5.4f};\".format(\n",
    "                epoch, t_loss, v_loss, t_accuracy, v_accuracy))\n",
    "\n",
    "    # Early Stopping\n",
    "    stopEarly = callback_early_stopping(valid_accuracies)\n",
    "    if stopEarly:\n",
    "        print(\"callback_early_stopping signal received at epoch= %d/%d\" %\n",
    "                (epoch, n_epochs))\n",
    "        print(\"Terminating training \")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model Test Accuracy: 52.53271%\n"
     ]
    }
   ],
   "source": [
    "_, predictions_test = standard_model(test_features)\n",
    "test_accuracy = accuracy(predictions_test, test_labels)\n",
    "print(\"Standard model Test Accuracy: {:.5f}%\".format(test_accuracy.numpy() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KENN model Test Accuracy: 61.15398%\n"
     ]
    }
   ],
   "source": [
    "ind_x = np.expand_dims(indexes_test[:,0], axis=1)\n",
    "ind_y = np.expand_dims(indexes_test[:,1], axis=1)\n",
    "\n",
    "predictions_test_kenn = kenn_model(\n",
    "    [test_features, relations_test, ind_x, ind_y])\n",
    "\n",
    "test_accuracy_kenn = accuracy(predictions_test_kenn, test_labels)\n",
    "print(\"KENN model Test Accuracy: {:.5f}%\".format(test_accuracy_kenn.numpy() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
